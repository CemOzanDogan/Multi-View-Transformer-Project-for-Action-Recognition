{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42796,"status":"ok","timestamp":1737482633899,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"zhw8Govb7wfF","outputId":"b4b01cd6-6d86-4cda-ee55-8149b0b47678"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":31078,"status":"ok","timestamp":1737482664976,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"VZzfd_sQ-X1O","outputId":"4d0593c0-4ec6-471a-96f1-d4c35dd89851"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/Top-10-Label-Dataset.zip'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import shutil\n","import os\n","\n","# Define the source directory (Google Drive) and destination directory (Colab session)\n","source_zip_path = '/content/drive/MyDrive/COMP411_Project_Datasets/Something-Something-V2/Top-10-Label-Dataset.zip'\n","destination_zip_path = '/content/Top-10-Label-Dataset.zip'\n","\n","shutil.copy(source_zip_path, destination_zip_path)"]},{"cell_type":"markdown","metadata":{"id":"_iRxN6CjQlNJ"},"source":["# USE WHEN NEEDED"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":11181,"status":"ok","timestamp":1737482676156,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"Jp3dRrGKAYbz","outputId":"57017366-c1a1-43a0-c01f-dfc3c5d5fe17"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/epoch_2_model'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import shutil\n","import os\n","\n","# Define the source directory (Google Drive) and destination directory (Colab session)\n","source_zip_path = '/content/drive/MyDrive/COMP411_Project_Models/1-layer-models/best_model_epoch_2.pth'\n","destination_zip_path = '/content/epoch_2_model'\n","\n","shutil.copy(source_zip_path, destination_zip_path)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1737482676156,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"Td9oi00put3_"},"outputs":[],"source":["import zipfile\n","import os\n","\n","def unzip_folder(zip_file_path, extract_to):\n","    \"\"\"\n","    Unzips a folder to the specified directory.\n","\n","    Args:\n","        zip_file_path (str): Path to the zip file.\n","        extract_to (str): Directory where the contents will be extracted.\n","    \"\"\"\n","    # Ensure the output directory exists\n","    os.makedirs(extract_to, exist_ok=True)\n","\n","    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_to)\n","        print(f\"Extracted all files to {extract_to}\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5507,"status":"ok","timestamp":1737482681660,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"l1yZuY3JuuW5","outputId":"d8280149-06b7-4359-8251-118d51fd778d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Extracted all files to /content/dataset/\n","Extracted all files to /content/data_labels/\n"]}],"source":["# Example usage\n","zip_file_path1 = '/content/Top-10-Label-Dataset.zip'  # Path to your zip file\n","extract_to1 = '/content/dataset/'  # Directory to extract files into\n","unzip_folder(zip_file_path1, extract_to1)\n","\n","\n","zip_file_path2 = '/content/data_labels.zip'  # Path to your zip file\n","extract_to2 = '/content/data_labels/'  # Directory to extract files into\n","unzip_folder(zip_file_path2, extract_to2)"]},{"cell_type":"markdown","metadata":{"id":"0ktLI0m2h7MV"},"source":["# LIBRARIES"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14045,"status":"ok","timestamp":1737482695703,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"x3qxf-SquIk6","outputId":"cb6acf1b-1f85-49f4-e02a-c7c2dbdf2929"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.13)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.20.1+cu121)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.27.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub-\u003etimm) (3.16.1)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub-\u003etimm) (2024.10.0)\n","Requirement already satisfied: packaging\u003e=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub-\u003etimm) (24.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub-\u003etimm) (2.32.3)\n","Requirement already satisfied: tqdm\u003e=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub-\u003etimm) (4.67.1)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub-\u003etimm) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (3.1.5)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (12.1.105)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch-\u003etimm) (1.13.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107-\u003etorch-\u003etimm) (12.6.85)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1-\u003etorch-\u003etimm) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision-\u003etimm) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision-\u003etimm) (11.1.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2-\u003etorch-\u003etimm) (3.0.2)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.11/dist-packages (from requests-\u003ehuggingface_hub-\u003etimm) (3.4.1)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.11/dist-packages (from requests-\u003ehuggingface_hub-\u003etimm) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests-\u003ehuggingface_hub-\u003etimm) (2.3.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests-\u003ehuggingface_hub-\u003etimm) (2024.12.14)\n"]}],"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","import time  # Import time module for tracking time.\n","import torch.multiprocessing as mp\n","import cv2 # For dividing the video into frames.\n","from tqdm import tqdm # For better visuals while training the model.\n","from torch import Tensor\n","from typing import Optional, Tuple\n","from PIL import Image\n","\n","!pip install timm\n","import timm"]},{"cell_type":"markdown","metadata":{"id":"Rpc2P3nCZOGG"},"source":["# DATASET CLASS"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1737482695704,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"H1idozPLf-5K"},"outputs":[],"source":["import re\n","import torch\n","import numpy as np\n","import cv2\n","import os\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","\n","class TwentyBillionSomethingDataset(Dataset):\n","    def __init__(self, video_dir, data_set, label_mapping, totalViews=3, target_frames=48,\n","                 target_size=(224, 224), is_test=False, test_answers_dict=None):\n","        self.video_dir = video_dir\n","        self.label_mapping = label_mapping\n","        self.totalViews = totalViews\n","        self.target_frames = target_frames\n","        self.target_size = target_size\n","        self.is_test = is_test\n","\n","        # Define transforms for training mode\n","        self.train_transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.RandomHorizontalFlip(p=0.7),  # Increased flip probability\n","            transforms.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.2), # Stronger color jitter with hue\n","            transforms.RandomErasing(p=0.7, scale=(0.05, 0.6)), # Higher chance of larger erasures\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","            ])\n","\n","        # Define transforms for test mode - only essential preprocessing\n","        self.test_transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","\n","        # Get available video IDs\n","        available_ids = set(int(file.split('.')[0]) for file in os.listdir(video_dir))\n","\n","        if not is_test:\n","            self.data_set = [entry for entry in data_set if int(entry[\"id\"]) in available_ids]\n","        else:\n","            self.data_set = [\n","                {\"id\": video_id, \"label\": int(label)}\n","                for video_id, label in test_answers_dict.items()\n","                if int(video_id) in available_ids\n","            ]\n","\n","    def __getitem__(self, idx):\n","        entry = self.data_set[idx]\n","        video_id = entry[\"id\"]\n","\n","        if self.is_test:\n","            label = entry[\"label\"]\n","        else:\n","            template = entry[\"template\"]\n","            template = re.sub(r'[\\[\\]]', '', template).strip()\n","            if template not in self.label_mapping:\n","                raise KeyError(f\"Template '{template}' not found in label_mapping\")\n","            label = int(self.label_mapping[template])\n","\n","        # Load and process video\n","        video_path = os.path.join(self.video_dir, f\"{video_id}.webm\")\n","        video = self.load_and_transform_video(video_path)\n","\n","        # Create three identical views\n","        views = self.create_three_views(video)\n","\n","        return views, label\n","\n","    def __len__(self):\n","        return len(self.data_set)\n","\n","    def load_and_transform_video(self, video_path):\n","        \"\"\"Load video frames and apply transforms frame by frame.\"\"\"\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","\n","            # Preprocess frame\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            frame = cv2.resize(frame, self.target_size)\n","\n","            # Apply appropriate transform based on mode\n","            transform = self.test_transform if self.is_test else self.train_transform\n","            frame = transform(frame)  # Returns tensor of shape (C, H, W)\n","            frames.append(frame)\n","\n","        cap.release()\n","\n","        if not frames:\n","            # Create empty frame tensor with correct dimensions\n","            frames = torch.zeros((1, 3, *self.target_size), dtype=torch.float32)\n","        else:\n","            frames = torch.stack(frames)  # Shape: (T, C, H, W)\n","\n","        # Handle frame count\n","        if len(frames) \u003e self.target_frames:\n","            indices = np.linspace(0, len(frames)-1, self.target_frames, dtype=int)\n","            frames = frames[indices]\n","        elif len(frames) \u003c self.target_frames:\n","            padding = torch.zeros((\n","                self.target_frames - len(frames),\n","                3,\n","                *self.target_size\n","            ), dtype=frames.dtype)\n","            frames = torch.cat([frames, padding], dim=0)\n","\n","        return frames\n","\n","    def create_three_views(self, video):\n","        \"\"\"Create three identical views of the video.\"\"\"\n","        views = []\n","        for _ in range(self.totalViews):  # Loop self.totalViews times to create the views\n","            views.append(video.clone())\n","        return views\n"]},{"cell_type":"markdown","metadata":{"id":"w3zbOM6kAv6O"},"source":["# TOKENIZER FUNCTION"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1737482695704,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"xwvlV8_VAx6j"},"outputs":[],"source":["class VideoTokenizer(nn.Module):\n","    def __init__(self, tubelet_sizes, embed_dims, input_channels=3):\n","        super(VideoTokenizer, self).__init__()\n","        self.tubelet_sizes = tubelet_sizes\n","        self.embed_dims = embed_dims\n","        self.input_channels = input_channels\n","\n","        # Initialize projection layers for each view\n","        self.view_projs = nn.ModuleList([\n","            nn.Linear(t * h * w * input_channels, embed_dim)\n","            for (t, h, w), embed_dim in zip(tubelet_sizes, embed_dims)\n","        ])\n","\n","    def calculate_padding(self, dimension_size, tubelet_size):\n","        \"\"\"Calculate padding to make the dimension divisible by the tubelet size.\"\"\"\n","        if dimension_size % tubelet_size == 0:\n","            return 0\n","        return tubelet_size - (dimension_size % tubelet_size)\n","\n","    def pad_tensor(self, tensor, padding):\n","        \"\"\"Apply padding dynamically to time, height, and width dimensions.\"\"\"\n","        t_pad, h_pad, w_pad = padding\n","        return nn.functional.pad(tensor, (0, 0, 0, w_pad, 0, h_pad, 0, t_pad))\n","\n","    def create_positional_encoding(self, num_t, num_spatial_tokens, embed_dim):\n","        \"\"\"Create separate positional encodings for temporal and spatial tokens.\"\"\"\n","        # Temporal positional encoding\n","        temporal_pos = torch.arange(0, num_t, dtype=torch.float).unsqueeze(1)\n","        div_term_temporal = torch.exp(torch.arange(0, embed_dim // 2, dtype=torch.float) *\n","                                     (-torch.log(torch.tensor(10000.0)) / (embed_dim // 2)))\n","        pe_temporal = torch.zeros(num_t, embed_dim)\n","        pe_temporal[:, 0::2] = torch.sin(temporal_pos * div_term_temporal)\n","        pe_temporal[:, 1::2] = torch.cos(temporal_pos * div_term_temporal) # Corrected typo here\n","\n","        # Spatial positional encoding\n","        spatial_pos = torch.arange(0, num_spatial_tokens, dtype=torch.float).unsqueeze(1)\n","        div_term_spatial = torch.exp(torch.arange(0, embed_dim // 2, dtype=torch.float) *\n","                                     (-torch.log(torch.tensor(10000.0)) / (embed_dim // 2)))\n","        pe_spatial = torch.zeros(num_spatial_tokens, embed_dim)\n","        pe_spatial[:, 0::2] = torch.sin(spatial_pos * div_term_spatial)\n","        pe_spatial[:, 1::2] = torch.cos(spatial_pos * div_term_spatial)\n","\n","        return pe_temporal, pe_spatial\n","\n","\n","    def forward(self, views):\n","        \"\"\"\n","        Args:\n","            views: List of tensors, each with shape (batch_size, frames, height, width, channels)\n","\n","        Returns:\n","            all_tokens: List of tensors with shape (batch_size, num_t, num_spatial_tokens, embed_dim)\n","        \"\"\"\n","        assert len(views) == len(self.tubelet_sizes)\n","\n","        all_tokens = []\n","        for view, tubelet_size, embed_dim, view_proj in zip(views, self.tubelet_sizes, self.embed_dims, self.view_projs):\n","            t, h, w = tubelet_size\n","            view = view.float()\n","            # Reshape to get the correct dimensions in order.\n","            reshaped_view = view.permute(0, 1, 4, 3, 2)\n","            batch_size, frames, height, width, channels = reshaped_view.shape\n","\n","            # Calculate and apply padding\n","            t_pad = self.calculate_padding(frames, t)\n","            h_pad = self.calculate_padding(height, h)\n","            w_pad = self.calculate_padding(width, w)\n","            padded_view = self.pad_tensor(reshaped_view, (t_pad, h_pad, w_pad))\n","            _, padded_frames, padded_height, padded_width, _ = padded_view.shape\n","\n","            # Calculate dimensions\n","            num_t = padded_frames // t  # Temporal tokens\n","            num_spatial_tokens = (padded_height // h) * (padded_width // w)\n","\n","            # Reshape for tubelet extraction\n","            tubelets = padded_view.reshape(\n","                batch_size,\n","                num_t, t,  # Temporal dimension\n","                padded_height // h, h,\n","                padded_width // w, w,\n","                channels\n","            )\n","\n","\n","            # Flatten spatial tubelets\n","            tubelets = tubelets.reshape(\n","                batch_size,\n","                num_t,\n","                num_spatial_tokens,  # Combine height and width tubelets\n","                t * h * w * channels  # Features per tubelet\n","            )\n","\n","            #print(\"Before projection tubelets.shape: \", tubelets.shape)\n","            # Apply projection layer\n","            tokens = view_proj(tubelets)  # Shape: (batch_size, num_t, num_spatial_tokens, embed_dim)\n","            #print(\"After projection tubelets.shape: \", tokens.shape)\n","\n","            # Create and add positional encodings\n","            pe_temporal, pe_spatial = self.create_positional_encoding(num_t, num_spatial_tokens, embed_dim)\n","\n","            # Add positional encodings with correct broadcasting\n","            tokens = tokens + pe_temporal.unsqueeze(0).unsqueeze(2).to(tokens.device) + pe_spatial.unsqueeze(0).unsqueeze(1).to(tokens.device)\n","\n","            all_tokens.append(tokens)\n","\n","        return all_tokens"]},{"cell_type":"markdown","metadata":{"id":"ZQDo0VGyC_Qy"},"source":["# LAYER ATTENTION"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1737482695704,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"8YGE6uhHC-6J"},"outputs":[],"source":["class LayerAttention(nn.Module):\n","    def __init__(\n","        self,\n","        embed_dim_i: int,\n","        num_heads: int = 8,\n","        dropout: float = 0.1,\n","        batch_first: bool = True\n","    ):\n","        super().__init__()\n","\n","        assert embed_dim_i % num_heads == 0, f\"embed_dim_i ({embed_dim_i}) must be divisible by num_heads ({num_heads})\"\n","\n","        self.embed_dim_i = embed_dim_i\n","        self.num_heads = num_heads\n","        self.dropout = dropout\n","        self.head_dim = embed_dim_i // num_heads\n","\n","        # Linear projections\n","        self.k_proj = nn.Linear(embed_dim_i, embed_dim_i)\n","        self.v_proj = nn.Linear(embed_dim_i, embed_dim_i)\n","        self.W_q_proj = nn.Linear(embed_dim_i, embed_dim_i)\n","        self.out_proj = nn.Linear(embed_dim_i, embed_dim_i)\n","\n","    def forward(\n","        self,\n","        view_i: Tensor,\n","        key_padding_mask: Optional[Tensor] = None,\n","        need_weights: bool = False\n","    ) -\u003e Tuple[Tensor, Optional[Tensor]]:\n","\n","        batch_size, seq_len, _ = view_i.shape\n","\n","        # Project queries, keys, and values\n","        q = self.W_q_proj(view_i)\n","        k = self.k_proj(view_i)\n","        v = self.v_proj(view_i)\n","\n","        # Reshape for multi-head attention\n","        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","\n","        # Scaled dot-product attention\n","        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n","\n","        if key_padding_mask is not None:\n","            attn_scores = attn_scores.masked_fill(\n","                key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf')\n","            )\n","\n","        attn_weights = F.softmax(attn_scores, dim=-1)\n","        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n","\n","        # Weighted sum of values\n","        attn_output = torch.matmul(attn_weights, v)\n","\n","        # Reshape and project output\n","        attn_output = attn_output.transpose(1, 2).contiguous().view(\n","            batch_size, seq_len, self.embed_dim_i\n","        )\n","        output = self.out_proj(attn_output)\n","\n","        if need_weights:\n","            return output, attn_weights\n","        return output, None"]},{"cell_type":"markdown","metadata":{"id":"i8JqGUmyJ63-"},"source":["# CROSS VIEW ATTENTION PART"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1737482695704,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"59hJTFhrJ7Nv"},"outputs":[],"source":["# All cros-view attentions' dropout=0.1(Subject to change).\n","class CrossViewAttention(nn.Module):\n","    def __init__(\n","        self,\n","        embed_dim_i: int,\n","        embed_dim_i_plus_1: int,\n","        num_heads: int = 6,\n","        dropout: float = 0.1,\n","        batch_first: bool = True\n","    ):\n","        \"\"\"\n","        Initialize Cross-View Attention module.\n","\n","        Args:\n","            embed_dim_i (int): Embedding dimension of view i\n","            embed_dim_i_plus_1 (int): Embedding dimension of view i+1\n","            num_heads (int): Number of attention heads\n","            dropout (float): Dropout probability\n","            batch_first (bool): If True, input shape is (batch, seq, feature)\n","        \"\"\"\n","        super().__init__()\n","\n","        assert embed_dim_i % num_heads == 0, f\"embed_dim_i ({embed_dim_i}) must be divisible by num_heads ({num_heads})\"\n","\n","        self.embed_dim_i = embed_dim_i\n","        self.embed_dim_i_plus_1 = embed_dim_i_plus_1\n","        self.num_heads = num_heads\n","        self.dropout = dropout\n","        self.batch_first = batch_first\n","        self.head_dim = embed_dim_i // num_heads\n","        self.scaling = self.head_dim ** -0.5\n","\n","        # Linear projection for K and V\n","        self.k_proj = nn.Linear(embed_dim_i_plus_1, embed_dim_i)\n","        self.v_proj = nn.Linear(embed_dim_i_plus_1, embed_dim_i)\n","\n","        # Linear projections\n","        self.W_q_proj = nn.Linear(embed_dim_i, embed_dim_i)\n","        self.W_k_proj = nn.Linear(embed_dim_i, embed_dim_i)\n","        self.W_v_proj = nn.Linear(embed_dim_i, embed_dim_i)\n","        self.out_proj = nn.Linear(embed_dim_i, embed_dim_i)\n","\n","        # Dropout for attention weights\n","        self.dropout_layer = nn.Dropout(dropout)\n","\n","        # Layer normalization for both views\n","        self.norm_i = nn.LayerNorm(embed_dim_i)\n","        self.norm_i_plus_1 = nn.LayerNorm(embed_dim_i_plus_1)\n","\n","    def forward(\n","        self,\n","        view_i: Tensor,\n","        view_i_plus_1: Tensor,\n","        key_padding_mask: Optional[Tensor] = None,\n","        need_weights: bool = False\n","    ) -\u003e Tuple[Tensor, Optional[Tensor]]:\n","        \"\"\"\n","        Apply cross-view attention between two views.\n","\n","        Args:\n","            view_i (Tensor): Tokens from view i, shape (batch_size, seq_len_i, embed_dim_i)\n","            view_i_plus_1 (Tensor): Tokens from view i+1, shape (batch_size, seq_len_i_plus_1, embed_dim_i_plus_1)\n","            key_padding_mask (Optional[Tensor]): Mask for padding tokens\n","            need_weights (bool): If True, returns attention weights\n","\n","        Returns:\n","            Tuple[Tensor, Optional[Tensor]]: (attended_tokens, attention_weights if need_weights else None)\n","        \"\"\"\n","        view_i = self.norm_i(view_i)\n","        view_i_plus_1 = self.norm_i_plus_1(view_i_plus_1)\n","\n","        batch_size, seq_len_i, _ = view_i.shape\n","        _, seq_len_i_plus_1, _ = view_i_plus_1.shape\n","\n","        k_projected = self.k_proj(view_i_plus_1)\n","        v_projected = self.v_proj(view_i_plus_1)\n","\n","        # Project to queries, keys, and values\n","        q = self.W_q_proj(view_i)\n","        k = self.W_k_proj(k_projected)\n","        v = self.W_v_proj(v_projected)\n","\n","        # Reshape for multi-head attention\n","        q = q.view(batch_size, seq_len_i, self.num_heads, self.head_dim).transpose(1, 2)\n","        k = k.view(batch_size, seq_len_i_plus_1, self.num_heads, self.head_dim).transpose(1, 2)\n","        v = v.view(batch_size, seq_len_i_plus_1, self.num_heads, self.head_dim).transpose(1, 2)\n","\n","        # Scaled dot-product attention\n","        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scaling\n","\n","        # Apply key padding mask if provided\n","        if key_padding_mask is not None:\n","            attn_weights = attn_weights.masked_fill(\n","                key_padding_mask.unsqueeze(1).unsqueeze(2),\n","                float('-inf')\n","            )\n","\n","        attn_weights = torch.softmax(attn_weights, dim=-1)\n","        attn_weights = self.dropout_layer(attn_weights)\n","\n","        # Apply attention weights to values\n","        attn_output = torch.matmul(attn_weights, v)\n","\n","        # Reshape and project output\n","        attn_output = attn_output.transpose(1, 2).contiguous().view(\n","            batch_size, seq_len_i, self.embed_dim_i\n","        )\n","        output = self.out_proj(attn_output)\n","\n","        # Residual connection\n","        output = output + view_i\n","\n","        if need_weights:\n","            return output, attn_weights\n","        return output, None"]},{"cell_type":"markdown","metadata":{"id":"VxRzsiPoGkeR"},"source":["# VIEW LAYER"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1737482695704,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"CtdblqjGN5ai"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","# Default dropout is 0.1 but given different dropout values for each view. Views have different complexities.\n","class ViewLayer(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, cross_attention_module = None,\n","                 crossAttentionCalculationNeeded = False, MLP_dim = 256, attention_head_num=8):\n","        \"\"\"\n","        Args:\n","            d_model: Input feature dimension\n","            dim_feedforward: Dimension of feedforward MLP\n","            dropout: Dropout probability\n","        \"\"\"\n","        super(ViewLayer, self).__init__()\n","\n","        self.dropout_value = dropout\n","\n","        self.crossAttentionCalculationNeeded = crossAttentionCalculationNeeded\n","        self.self_attention = LayerAttention(embed_dim_i=d_model, dropout=self.dropout_value, num_heads=attention_head_num)  # Self-attention\n","        self.cross_attention = cross_attention_module\n","\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)  # For FFN residual\n","\n","        self.linear1 = nn.Linear(d_model, MLP_dim)  # First linear layer\n","        self.linear2 = nn.Linear(MLP_dim, d_model)  # Second linear layer\n","\n","        self.dropout1 = nn.Dropout(self.dropout_value)\n","        self.dropout2 = nn.Dropout(self.dropout_value)\n","\n","        self.mid_processed_tokens = None  # Store intermediate tokens\n","\n","\n","    def forward(self, src, mid_tokens_from_other_view=None, src_mask=None):\n","        \"\"\"\n","        Args:\n","            src: Input tensor (batch_size, seq_len, d_model)\n","            src_mask: Optional attention mask\n","        \"\"\"\n","        src2 = self.norm1(src)\n","        src2, _ = self.self_attention(src2) # Self attention. Only takes one input for self-attention.\n","        src = src + self.dropout1(src2)\n","\n","\n","        self.mid_processed_tokens = src # Store intermediate tokens\n","\n","        # Cross-Attention (if needed)\n","        if self.crossAttentionCalculationNeeded and mid_tokens_from_other_view is not None:\n","            src2, _ = self.cross_attention(self.norm1(self.mid_processed_tokens), mid_tokens_from_other_view) # view_i and view_i_plus_one\n","            src = src + self.dropout1(src2)\n","\n","        src2 = self.norm2(src)\n","        src3 = self.linear2(F.relu(self.linear1(src2)))\n","        src = src + self.dropout2(src3)\n","\n","        return src\n","\n","    def get_mid_processed_tokens(self):\n","      return self.mid_processed_tokens\n","\n","    def load_pretrained_weights_for_layer(self, weights):\n","        \"\"\"\n","        Load pretrained weights from a ViT model into the ViewLayer with comprehensive error checking.\n","\n","        Args:\n","            weights (dict): Dictionary containing pretrained weights from ViT layer\n","\n","        Raises:\n","            ValueError: If weights are missing or dimensions don't match\n","            RuntimeError: If tensor device or dtype mismatches occur\n","        \"\"\"\n","        required_keys = {\n","            'self_attn.qkv.weight', 'self_attn.qkv.bias',\n","            'self_attn.proj.weight', 'self_attn.proj.bias',\n","            'norm1.weight', 'norm1.bias',\n","            'norm2.weight', 'norm2.bias',\n","            'mlp.fc1.weight', 'mlp.fc1.bias',\n","            'mlp.fc2.weight', 'mlp.fc2.bias'\n","        }\n","\n","        # Check if all required weights are present\n","        missing_keys = required_keys - set(weights.keys())\n","        if missing_keys:\n","            raise ValueError(f\"Missing required weights: {missing_keys}\")\n","\n","        # Get model dimensions for validation\n","        embed_dim = self.self_attention.embed_dim_i\n","        mlp_dim = self.linear1.out_features  # MLP hidden dimension\n","\n","        try:\n","            # Validate QKV dimensions\n","            qkv_weight = weights['self_attn.qkv.weight']\n","            qkv_bias = weights['self_attn.qkv.bias']\n","\n","            expected_qkv_weight_shape = (3 * embed_dim, embed_dim)\n","            expected_qkv_bias_shape = (3 * embed_dim,)\n","\n","            if qkv_weight.shape != expected_qkv_weight_shape:\n","                raise ValueError(\n","                    f\"QKV weight shape mismatch. Expected {expected_qkv_weight_shape}, \"\n","                    f\"got {qkv_weight.shape}\"\n","                )\n","            if qkv_bias.shape != expected_qkv_bias_shape:\n","                raise ValueError(\n","                    f\"QKV bias shape mismatch. Expected {expected_qkv_bias_shape}, \"\n","                    f\"got {qkv_bias.shape}\"\n","                )\n","\n","            # Split and validate Q, K, V shapes\n","            qkv_weight = qkv_weight.reshape(3, embed_dim, embed_dim)\n","            qkv_bias = qkv_bias.reshape(3, embed_dim)\n","\n","            # Validate projection dimensions\n","            if weights['self_attn.proj.weight'].shape != (embed_dim, embed_dim):\n","                raise ValueError(\n","                    f\"Projection weight shape mismatch. Expected ({embed_dim}, {embed_dim}), \"\n","                    f\"got {weights['self_attn.proj.weight'].shape}\"\n","                )\n","\n","            # Validate norm dimensions\n","            for norm_key in ['norm1.weight', 'norm1.bias', 'norm2.weight', 'norm2.bias']:\n","                if weights[norm_key].shape != (embed_dim,):\n","                    raise ValueError(\n","                        f\"Norm weight/bias shape mismatch for {norm_key}. \"\n","                        f\"Expected ({embed_dim},), got {weights[norm_key].shape}\"\n","                    )\n","\n","            # Validate MLP dimensions\n","            if weights['mlp.fc1.weight'].shape != (mlp_dim, embed_dim):\n","                raise ValueError(\n","                    f\"MLP fc1 weight shape mismatch. Expected ({mlp_dim}, {embed_dim}), \"\n","                    f\"got {weights['mlp.fc1.weight'].shape}\"\n","                )\n","            if weights['mlp.fc2.weight'].shape != (embed_dim, mlp_dim):\n","                raise ValueError(\n","                    f\"MLP fc2 weight shape mismatch. Expected ({embed_dim}, {mlp_dim}), \"\n","                    f\"got {weights['mlp.fc2.weight'].shape}\"\n","                )\n","\n","            # Check devices and dtypes\n","            device = self.self_attention.W_q_proj.weight.device\n","            dtype = self.self_attention.W_q_proj.weight.dtype\n","\n","            # Load weights with device and dtype validation\n","            def load_tensor(tensor, target):\n","                if tensor.device != device:\n","                    tensor = tensor.to(device)\n","                if tensor.dtype != dtype:\n","                    tensor = tensor.to(dtype)\n","                target.data.copy_(tensor)\n","\n","            # Load Q, K, V weights and biases\n","            load_tensor(qkv_weight[0], self.self_attention.W_q_proj.weight)\n","            load_tensor(qkv_weight[1], self.self_attention.k_proj.weight)\n","            load_tensor(qkv_weight[2], self.self_attention.v_proj.weight)\n","\n","            load_tensor(qkv_bias[0], self.self_attention.W_q_proj.bias)\n","            load_tensor(qkv_bias[1], self.self_attention.k_proj.bias)\n","            load_tensor(qkv_bias[2], self.self_attention.v_proj.bias)\n","\n","            # Load projection weights\n","            load_tensor(weights['self_attn.proj.weight'], self.self_attention.out_proj.weight)\n","            load_tensor(weights['self_attn.proj.bias'], self.self_attention.out_proj.bias)\n","\n","            # Load normalization weights\n","            load_tensor(weights['norm1.weight'], self.norm1.weight)\n","            load_tensor(weights['norm1.bias'], self.norm1.bias)\n","            load_tensor(weights['norm2.weight'], self.norm2.weight)\n","            load_tensor(weights['norm2.bias'], self.norm2.bias)\n","\n","            # Load MLP weights\n","            load_tensor(weights['mlp.fc1.weight'], self.linear1.weight)\n","            load_tensor(weights['mlp.fc1.bias'], self.linear1.bias)\n","            load_tensor(weights['mlp.fc2.weight'], self.linear2.weight)\n","            load_tensor(weights['mlp.fc2.bias'], self.linear2.bias)\n","\n","        except RuntimeError as e:\n","            raise RuntimeError(f\"Error during weight loading: {str(e)}\")\n","        except Exception as e:\n","            raise ValueError(f\"Unexpected error during weight loading: {str(e)}\")"]},{"cell_type":"markdown","metadata":{"id":"0J67OcO1W2Tz"},"source":["# GLOBAL ENCODER PART"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1737482695704,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"YzRkRbmAW2jv"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class GlobalEncoder(nn.Module):\n","    def __init__(self, embed_dim, num_layers):\n","        super(GlobalEncoder, self).__init__()\n","        self.global_layers = nn.ModuleList([\n","            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8, dim_feedforward=embed_dim*4, dropout=0.1)\n","            for _ in range(num_layers)\n","        ])\n","\n","    def forward(self, tokens):\n","        \"\"\"\n","        Parameters:\n","            tokens: Input tensor of shape (batch_size, seq_length, embed_dim) or (seq_length, batch_size, embed_dim)\n","        Returns:\n","            output: Tensor of same shape as input\n","        \"\"\"\n","        # Check if input needs to be transposed\n","        if tokens.dim() == 3 and tokens.size(2) == self.global_layers[0].linear1.in_features:\n","            # Input is (batch_size, seq_length, embed_dim)\n","            # Transform to (seq_length, batch_size, embed_dim)\n","            tokens = tokens.transpose(0, 1)\n","            need_transpose = True\n","        else:\n","            need_transpose = False\n","\n","        # Create attention mask for padding if needed\n","        # Assuming no padding mask for now, but you can add:\n","        # mask = torch.ones((tokens.size(0), tokens.size(0))).bool().to(tokens.device)\n","\n","        # Process through transformer layers\n","        for layer in self.global_layers:\n","            tokens = layer(tokens)\n","\n","        # Restore original shape if needed\n","        if need_transpose:\n","            tokens = tokens.transpose(0, 1)\n","\n","        return tokens\n","\n","    @staticmethod # NOT USED. IF NEEDED USE IT.\n","    def create_pad_mask(seq_length, valid_lens, device):\n","        \"\"\"\n","        Create padding mask for sequences of different lengths\n","\n","        Parameters:\n","            seq_length: Maximum sequence length\n","            valid_lens: Tensor of shape (batch_size,) containing valid lengths of each sequence\n","            device: Device to create mask on\n","        Returns:\n","            mask: Boolean mask tensor of shape (seq_length, seq_length)\n","        \"\"\"\n","        mask = torch.ones((seq_length, seq_length), device=device)\n","        mask = torch.triu(mask) == 1\n","        return mask"]},{"cell_type":"markdown","metadata":{"id":"1StVFgBhKBWW"},"source":["# MULTIVIEW TRANSFORMER PART"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1737482695704,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"84Ttkx7hKBpQ"},"outputs":[],"source":["class MultiviewTransformer(nn.Module):\n","    def __init__(self, device, embed_dims_per_view, MLP_dims_per_view, num_of_head_per_view,\n","                 view_dropout_list, num_layers_per_view, tubelet_sizes, global_embed_dim,\n","                 global_num_layers, num_classes, cross_view_connections):\n","        super(MultiviewTransformer, self).__init__()\n","\n","        self.tubelet_sizes = tubelet_sizes\n","        self.embed_dims_per_view = embed_dims_per_view\n","        self.cross_view_connections = cross_view_connections\n","        self.num_layers_per_view = num_layers_per_view\n","        self.view_dropout_list = view_dropout_list\n","\n","        # Tokenizer for video frames into tubelets\n","        self.tokenizer = VideoTokenizer(tubelet_sizes, embed_dims_per_view)\n","        \"\"\"\n","        # Initialize transformers for each view (Base, Small, Tiny)\n","        self.view_transformers = nn.ModuleList([\n","            ViewTransformer(embed_dim=embed_dim, num_layers=num_layers)\n","            for embed_dim, num_layers in zip(embed_dims_per_view, num_layers_per_view)\n","        ])\n","\n","\n","        nn.TransformerEncoderLayer(d_model=embed_dims_per_view[view], nhead=8,\n","                                     dim_feedforward=512, dropout=0.1,\n","                                     batch_first=True)  # Set batch_first=True\n","        \"\"\"\n","        # Initialize Cross-View Attention layers\n","        \"\"\"\n","        self.cross_attention_layers = nn.ModuleList([\n","            CrossViewAttention(in_dim, out_dim).to(device)\n","            for in_dim, out_dim in embed_dims_pairs_for_cross_att\n","            ])\n","        \"\"\"\n","        #print(self.cross_attention_layers)\n","\n","        # list containing layers of different views as its elements. Do the crossAttentionCalculationNeeded check for cross attention calculations.\n","        self.total_view_num = len(self.embed_dims_per_view)\n","        \"\"\"\n","        self.cls_token = nn.ParameterList([\n","            nn.Parameter(torch.zeros(1, 1, embed_dims_per_view[cls_num]))\n","            for cls_num in range(self.total_view_num)\n","            ])\n","        \"\"\"\n","        # Better initialization(Kaiming/He)\n","        self.cls_tokens = nn.ParameterList([\n","            nn.Parameter(torch.nn.init.kaiming_normal_(\n","                torch.zeros(1, 1, 1, embed_dims_per_view[cls_num]),  # [1, 1, 1, E]\n","                mode='fan_out',\n","                nonlinearity='relu'\n","                ))\n","            for cls_num in range(len(embed_dims_per_view))\n","            ])\n","\n","        self.view_list = nn.ModuleList()  # Use ModuleList to store layers for each view\n","        for view in range(self.total_view_num):  # Iterate over views\n","            layer_list = nn.ModuleList()  # Use ModuleList to store layers for a specific view\n","            for layer_num in range(num_layers_per_view[view]):\n","                if layer_num in cross_view_connections and view != (self.total_view_num - 1):\n","                    layer_list.append(\n","                        ViewLayer(\n","                            d_model=embed_dims_per_view[view], dropout=self.view_dropout_list[view], # Setting the dropout for each view layers.\n","                            cross_attention_module=CrossViewAttention(embed_dims_per_view[view], embed_dims_per_view[view + 1]), # Initialize the cross view.\n","                            crossAttentionCalculationNeeded=True, MLP_dim=MLP_dims_per_view[view], attention_head_num=num_of_head_per_view[view]\n","                        ).to(device)\n","                    )\n","                else:\n","                    layer_list.append(\n","                        ViewLayer(\n","                            d_model=embed_dims_per_view[view], dropout=self.view_dropout_list[view],\n","                            crossAttentionCalculationNeeded=False,\n","                            MLP_dim=MLP_dims_per_view[view], attention_head_num=num_of_head_per_view[view]\n","                        ).to(device)\n","                    )\n","            self.view_list.append(layer_list)  # Add the per-view ModuleList to the overall ModuleList\n","\n","        # Linear projections to unify embedding dimensions\n","        self.projections = nn.ModuleList([\n","            nn.Linear(embed_dim, global_embed_dim) for embed_dim in embed_dims_per_view\n","        ])\n","\n","        self.global_encoder = GlobalEncoder(embed_dim=global_embed_dim, num_layers=global_num_layers)\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(global_embed_dim, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, num_classes)\n","        ).to(device)\n","\n","    def forward(self, videos):\n","        # Tokenize video frames into tubelets for each view\n","        # Returns list of tokens with shape [B, T, HW, E] for each view\n","        video_tokens_per_view = self.tokenizer(videos)\n","        reversed_tokens = list(reversed(video_tokens_per_view))\n","\n","        view_execution_order = list(reversed(self.view_list))\n","\n","        # Prepare CLS tokens for each view\n","        concatenated_tokens = []\n","        for cls_token, tokens in zip(reversed(self.cls_tokens), reversed_tokens):\n","            B, T, HW, E = tokens.shape\n","            # Expand CLS token to batch and temporal dimensions\n","            expanded_cls = cls_token.expand(B, T, 1, E)  # [B, T, 1, E]\n","            # Concatenate with spatial tokens\n","            concat_tokens = torch.cat((expanded_cls, tokens), dim=2)  # [B, T, 1+HW, E]\n","            concatenated_tokens.append(concat_tokens)\n","\n","        # Process through transformer layers\n","        tokens = []\n","        for layer in range(self.num_layers_per_view[0]):\n","            mid_tokens_view_i_plus_one = None\n","            layer_tokens = []\n","            for current_view, input_tokens in zip(view_execution_order, concatenated_tokens):\n","                # Reshape to sequence form for transformer processing\n","                B, T, N, E = input_tokens.shape\n","                reshaped_tokens = input_tokens.reshape(B, T * N, E)\n","\n","                if layer in self.cross_view_connections:\n","                    processed_token = current_view[layer](reshaped_tokens, mid_tokens_view_i_plus_one)\n","                    mid_tokens_view_i_plus_one = current_view[layer].get_mid_processed_tokens()\n","                else:\n","                    processed_token = current_view[layer](reshaped_tokens)\n","\n","                # Reshape back to temporal form\n","                processed_token = processed_token.reshape(B, T, N, E)\n","                layer_tokens.append(processed_token)\n","            tokens.append(layer_tokens)\n","\n","        new_tokens = list(reversed(tokens[self.num_layers_per_view[0] - 1]))\n","\n","        # Extract and process temporal CLS tokens\n","        processed_cls_tokens = []\n","        for i in range(self.total_view_num):\n","            # Extract CLS tokens (first token of each temporal step)\n","            cls_tokens = new_tokens[i][:, :, 0, :]  # Shape: [B, T, E]\n","            # Project the CLS tokens\n","            projected_cls = self.projections[i](cls_tokens)  # Shape: [B, T, global_embed_dim]\n","            processed_cls_tokens.append(projected_cls)\n","\n","        # Combine temporal CLS tokens from all views\n","        combined_cls_tokens = torch.cat(processed_cls_tokens, dim=1)  # Shape: [B, T_total, global_embed_dim]\n","\n","        # Process through global encoder and classifier\n","        global_encoder_output = self.global_encoder(combined_cls_tokens)\n","        pooled_output = torch.mean(global_encoder_output, dim=1)\n","        final_output = self.classifier(pooled_output)\n","\n","        return final_output\n","\n","    \"\"\"\n","    pretrained_weights_list =\n","    \"\"\"\n","    def usePretrainedWeights(self, pretrained_layer_list_for_each_view):\n","        for view_config in pretrained_layer_list_for_each_view:\n","            view_idx = view_config[0]\n","            layer_mappings = view_config[1]\n","\n","            for our_layer_num, vit_layer_num in layer_mappings:\n","                print(f\"Loading pretrained weights for view {view_idx}, layer {our_layer_num}\")\n","                pretrained_weights = self.get_pretrained_weights_for_layer_from_ViT(view_idx, vit_layer_num)\n","                self.view_list[view_idx][our_layer_num].load_pretrained_weights_for_layer(pretrained_weights)\n","\n","    def get_pretrained_weights_for_layer_from_ViT(self, view_idx, ViT_layer_num):\n","        \"\"\"\n","        Get pretrained weights from the appropriate ViT model based on view size.\n","\n","        Args:\n","            view_idx (int): Index of the view (0: Base/12 layers, 1: Small/8 layers, 2: Tiny/4 layers)\n","            ViT_layer_num (int): Layer number from the ViT model to fetch weights from\n","\n","        Returns:\n","            dict: Pretrained weights for the specified layer\n","        \"\"\"\n","        # Define model configurations\n","        vit_configs = {\n","            0: {'name': 'vit_base_patch16_224', 'num_layers': 12},  # Largest view uses ViT-Base\n","            1: {'name': 'vit_small_patch16_224', 'num_layers': 12},  # Middle view uses ViT-Small\n","            2: {'name': 'vit_tiny_patch16_224', 'num_layers': 12}    # Smallest view uses ViT-Tiny\n","        }\n","\n","        if view_idx not in vit_configs:\n","            raise ValueError(f\"Invalid view_idx: {view_idx}. Must be 0 (Base), 1 (Small), or 2 (Tiny)\")\n","\n","        config = vit_configs[view_idx]\n","\n","        # Validate the requested layer number\n","        if ViT_layer_num \u003e= config['num_layers']:\n","            raise ValueError(\n","                f\"Invalid layer number {ViT_layer_num} for ViT-{config['name']}. \"\n","                f\"This model only has {config['num_layers']} layers (0-{config['num_layers']-1})\"\n","            )\n","\n","        # Load the appropriate model\n","        model = timm.create_model(config['name'], pretrained=True)\n","        encoder_layer = model.blocks[ViT_layer_num]\n","\n","        # Extract weights\n","        weights = {\n","            'self_attn.qkv.weight': encoder_layer.attn.qkv.weight,\n","            'self_attn.qkv.bias': encoder_layer.attn.qkv.bias,\n","            'self_attn.proj.weight': encoder_layer.attn.proj.weight,\n","            'self_attn.proj.bias': encoder_layer.attn.proj.bias,\n","            'mlp.fc1.weight': encoder_layer.mlp.fc1.weight,\n","            'mlp.fc1.bias': encoder_layer.mlp.fc1.bias,\n","            'mlp.fc2.weight': encoder_layer.mlp.fc2.weight,\n","            'mlp.fc2.bias': encoder_layer.mlp.fc2.bias,\n","            'norm1.weight': encoder_layer.norm1.weight,\n","            'norm1.bias': encoder_layer.norm1.bias,\n","            'norm2.weight': encoder_layer.norm2.weight,\n","            'norm2.bias': encoder_layer.norm2.bias\n","        }\n","\n","        return weights"]},{"cell_type":"markdown","metadata":{"id":"OH9jC39hKKXR"},"source":["# GET THE DATA LABELS"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1324,"status":"ok","timestamp":1737482697023,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"jcNGXG9lKKIs","outputId":"0d53f206-7681-4dc4-8416-746994af7c15"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Summary: [{'id': '78687', 'label': 'holding potato next to vicks vaporub bottle', 'template': 'Holding [something] next to [something]'}, {'id': '42326', 'label': 'spreading margarine onto bread', 'template': 'Spreading [something] onto [something]'}, {'id': '100904', 'label': 'putting pen on a surface', 'template': 'Putting [something] on a surface'}, {'id': '80715', 'label': 'lifting up one end of bottle, then letting it drop down', 'template': 'Lifting up one end of [something], then letting it drop down'}, {'id': '34899', 'label': 'holding bulb', 'template': 'Holding [something]'}]\n","Validation Summary: [{'id': '74225', 'label': 'spinning cube that quickly stops spinning', 'template': 'Spinning [something] that quickly stops spinning'}, {'id': '116154', 'label': 'showing clay box on top of wallet', 'template': 'Showing [something] on top of [something]'}, {'id': '198186', 'label': 'wiping words off of a paper', 'template': 'Wiping [something] off of [something]'}, {'id': '137878', 'label': 'pushing scissors so that it falls off the table', 'template': 'Pushing [something] so that it falls off the table'}, {'id': '151151', 'label': 'turning the camera left while filming wall mounted fan', 'template': 'Turning the camera left while filming [something]'}]\n","Test Summary: [{'id': '1420'}, {'id': '166429'}, {'id': '53930'}, {'id': '73548'}, {'id': '142328'}]\n","Test Answers Sample: {'208583': '42', '50058': '144', '139625': '114', '40500': '122', '79958': '35'}\n"]}],"source":["import csv\n","import json\n","import os\n","\n","def load_json(file_path):\n","    \"\"\"Utility to load a JSON file.\"\"\"\n","    if not os.path.exists(file_path):\n","        raise FileNotFoundError(f\"File not found: {file_path}\")\n","    with open(file_path, \"r\") as f:\n","        return json.load(f)\n","\n","def load_csv(file_path, delimiter=\";\"):\n","    \"\"\"Utility to load a CSV file.\"\"\"\n","    if not os.path.exists(file_path):\n","        raise FileNotFoundError(f\"File not found: {file_path}\")\n","    with open(file_path, \"r\") as f:\n","        reader = csv.reader(f, delimiter=delimiter)\n","        return list(reader)\n","\n","def map_test_answers(file_path, label_mapping):\n","    \"\"\"Maps test answers from CSV to numerical class IDs.\"\"\"\n","    test_answers = {}\n","    rows = load_csv(file_path)\n","    for row in rows:\n","        if len(row) == 2:  # Ensure the row has exactly two elements (video_id and label)\n","            video_id, label = row\n","            if label in label_mapping:  # Ensure label exists in the mapping\n","                test_answers[video_id] = label_mapping[label]\n","            else:\n","                print(f\"Warning: Label '{label}' not found in label_mapping\")\n","        else:\n","            print(f\"Skipping row due to incorrect format: {row}\")\n","    return test_answers\n","\n","# Paths to files\n","base_path = \"/content/data_labels/20bn-data-labels/labels\"\n","label_file = os.path.join(base_path, \"labels.json\")\n","train_file = os.path.join(base_path, \"train.json\")\n","val_file = os.path.join(base_path, \"validation.json\")\n","test_file = os.path.join(base_path, \"test.json\")\n","test_answers_file = os.path.join(base_path, \"test-answers.csv\")\n","\n","# Load label mapping and datasets\n","label_mapping = load_json(label_file)\n","train_data = load_json(train_file)\n","val_data = load_json(val_file)\n","test_data = load_json(test_file)\n","\n","# Map test answers to class IDs\n","test_answers = map_test_answers(test_answers_file, label_mapping)\n","\n","# Summarize datasets\n","def summarize_dataset(dataset, keys, num_samples=5):\n","    \"\"\"Creates a summary of the first few entries of a dataset.\"\"\"\n","    return [{key: entry.get(key, None) for key in keys} for entry in dataset[:num_samples]]\n","\n","train_summary = summarize_dataset(train_data, [\"id\", \"label\", \"template\"])\n","val_summary = summarize_dataset(val_data, [\"id\", \"label\", \"template\"])\n","test_summary = summarize_dataset(test_data, [\"id\"])\n","test_answers_summary = {k: test_answers[k] for k in list(test_answers)[:5]}\n","\n","# Display summaries\n","print(\"Train Summary:\", train_summary)\n","print(\"Validation Summary:\", val_summary)\n","print(\"Test Summary:\", test_summary)\n","print(\"Test Answers Sample:\", test_answers_summary)"]},{"cell_type":"markdown","metadata":{"id":"jhaPQefMgx3q"},"source":["# TRAIN CODE"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1737482697023,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"R20L00yrgyJR"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import time\n","from tqdm import tqdm\n","\n","def train_model(model, dataloader, criterion, optimizer, device,\n","                current_epoch_num, save_dir, total_num_epochs,\n","                label_mapping, scheduler=None, accumulation_steps=1):\n","    \"\"\"\n","    Train function with gradient accumulation (normal precision).\n","    \"\"\"\n","\n","    model.to(device)\n","    model.train()\n","\n","    epoch_start_time = time.time()\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_samples = 0\n","    optimizer.zero_grad()  # Move this outside the loop for accumulation\n","\n","    total_batches = len(dataloader)\n","\n","    with tqdm(enumerate(dataloader), total=total_batches, desc=f\"Epoch {current_epoch_num + 1}/{total_num_epochs}\") as pbar:\n","        for batch_idx, (inputs, targets) in pbar:\n","            # Move inputs and targets to the device\n","            inputs = [video.to(device) if isinstance(video, torch.Tensor) else torch.tensor(video).to(device) for video in inputs]\n","            targets = targets.to(device)\n","\n","            try:\n","                # Forward pass and loss computation\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","                loss = loss / accumulation_steps  # Normalize loss by accumulation steps\n","            except Exception as e:\n","                print(f\"Error in forward pass: {e}\")\n","                raise\n","\n","            # Perform backward pass\n","            loss.backward()\n","\n","            # Clip gradients for stability\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","            # Accumulate gradients for `accumulation_steps` before updating weights\n","            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(dataloader):\n","                optimizer.step()  # Update weights\n","                optimizer.zero_grad()  # Reset gradients after step\n","\n","            # Update running loss\n","            running_loss += loss.item() * accumulation_steps  # Scale back to original loss\n","\n","            # Calculate training accuracy\n","            _, predicted = torch.max(outputs, 1)\n","            correct_predictions += (predicted == targets).sum().item()\n","            total_samples += targets.size(0)\n","\n","            # Update tqdm progress bar\n","            pbar.set_postfix({\n","                'loss': running_loss / (batch_idx + 1),\n","                'accuracy': (correct_predictions / total_samples) * 100\n","            })\n","\n","    # Calculate average loss and accuracy for the epoch\n","    avg_loss = running_loss / len(dataloader)\n","    epoch_accuracy = (correct_predictions / total_samples) * 100\n","    epoch_time = time.time() - epoch_start_time\n","\n","    print(f\"################## Train Epoch Summary [{current_epoch_num + 1}/{total_num_epochs}]##################:\")\n","    print(f\"Train Average Loss: {avg_loss:.4f}\")\n","    print(f\"Accuracy: {epoch_accuracy:.2f}%\")\n","    print(f\"Time: {epoch_time:.2f}s\")\n","\n","    return avg_loss, epoch_accuracy"]},{"cell_type":"markdown","metadata":{"id":"gg9qEN7tgybq"},"source":["# TEST CODE"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1737482697023,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"SNoLFnvngypE"},"outputs":[],"source":["# Test code\n","from tqdm import tqdm\n","\n","def test_model(model, dataloader, device, label_mapping=None, criterion=None):\n","    \"\"\"\n","    Evaluate the model's accuracy on the test set (normal precision).\n","\n","    Args:\n","        model: The trained PyTorch model.\n","        dataloader: DataLoader for the test set.\n","        device: Device to run the model on (e.g., 'cpu' or 'cuda').\n","        label_mapping (optional): A mapping of class indices to labels.\n","        criterion (optional): A loss function to compute average loss during evaluation.\n","\n","    Returns:\n","        avg_loss: The average loss over the test set.\n","        accuracy: The overall test accuracy as a percentage.\n","    \"\"\"\n","    model.to(device)\n","    model.eval()\n","\n","    correct = 0\n","    total = 0\n","    running_loss = 0.0  # Initialize running loss\n","\n","    total_batches = len(dataloader)\n","\n","    with torch.no_grad():\n","        with tqdm(enumerate(dataloader), total=total_batches, desc=\"Testing\") as pbar:\n","            for batch_idx, (inputs, targets) in pbar:\n","\n","                # Move inputs and targets to the appropriate device\n","                if isinstance(inputs, list):  # For multi-view inputs\n","                    inputs = [video.to(device) for video in inputs]\n","                else:\n","                    inputs = inputs.to(device)\n","\n","                targets = targets.to(device)\n","\n","                try:\n","                    # Forward pass\n","                    outputs = model(inputs)\n","                except Exception as e:\n","                    print(f\"Error in forward pass: {e}\")\n","                    raise\n","\n","                # Get class indices of max scores\n","                _, predicted = torch.max(outputs, dim=1)\n","                total += targets.size(0)  # Accumulate total samples\n","                correct += (predicted == targets).sum().item()  # Accumulate correct predictions\n","\n","                if criterion:\n","                    loss = criterion(outputs, targets)\n","                    running_loss += loss.item()  # Accumulate loss\n","\n","                # Update tqdm progress bar\n","                pbar.set_postfix({\n","                    'loss': running_loss / (batch_idx + 1) if criterion else \"N/A\",\n","                    'accuracy': (correct / total) * 100\n","                })\n","\n","    # Calculate overall accuracy and average loss\n","    accuracy = correct / total if total \u003e 0 else 0.0\n","    avg_loss = running_loss / len(dataloader) if len(dataloader) \u003e 0 else 0.0\n","\n","    # Print results\n","    print(f'\\nTest Results for Epoch:')\n","    print(f'Total Samples: {total}')\n","    print(f'Correct Predictions: {correct}')\n","    print(f'Test Average Loss: {avg_loss:.4f}')\n","    print(f'Accuracy: {accuracy * 100:.2f}%')\n","\n","    return avg_loss, accuracy * 100"]},{"cell_type":"markdown","metadata":{"id":"HZ1U0BAOZOGK"},"source":["# TRAIN-TEST TOGETHER FUNCTION"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1737482697024,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"6JHyxRtJZOGK"},"outputs":[],"source":["import os\n","import torch\n","import time\n","\n","def train_then_test_for_each_epoch(\n","    model, train_loader, criterion, optimizer, device, total_num_epochs,\n","    test_loader, save_dir, label_mapping, scheduler, save_per_epoch,\n","    start_epoch, limit_epoch, early_stopping_patience=None, use_pretrained_weights=False\n","):\n","    \"\"\"\n","    pretrained_layer_list_for_each_view = [\n","            # View 0 (Base) - All 4 layers from ViT-Base\n","            [0, [[0,0], [1,1], [2,3], [3,5], [4,7], [5,8]]],\n","            # View 1 (Small) - All 4 layers from ViT-Small\n","            [1, [[0,0], [1,1], [2,3], [3,5], [4,7], [5,8]]],\n","            # View 2 (Tiny) - All 4 layers from ViT-Tiny\n","            [2, [[0,0], [1,1], [2,3], [3,5], [4,7], [5,8]]]\n","        ]\n","    \"\"\"\n","    if use_pretrained_weights:\n","        print(\"Using pretrained weights...\")\n","        pretrained_layer_list_for_each_view = [\n","            # View 0 (Base) - All 4 layers from ViT-Base\n","            [0, [[0,0]]],\n","            # View 1 (Small) - All 4 layers from ViT-Small\n","            [1, [[0,0]]],\n","            # View 2 (Tiny) - All 4 layers from ViT-Tiny\n","            [2, [[0,0]]]\n","        ]\n","        model.usePretrainedWeights(pretrained_layer_list_for_each_view)\n","\n","    train_epoch_accuracies = []\n","    test_epoch_accuracies = []\n","    best_test_accuracy = -float('inf')\n","    patience_counter = 0\n","    best_model_path = None\n","    total_start_time = time.time()\n","\n","    for epoch in range(start_epoch, total_num_epochs):\n","        if (epoch + 1) \u003c= limit_epoch:\n","            epoch_start_time = time.time()\n","            current_lr = optimizer.param_groups[0]['lr']\n","            print(f\"\\nEpoch {epoch + 1}/{total_num_epochs}  Current Learning Rate: {current_lr:.6f}\")\n","\n","            print(\"Training...\")\n","            train_epoch_loss, train_epoch_accuracy = train_model(\n","                model, train_loader, criterion, optimizer, device, (epoch),\n","                save_dir, total_num_epochs, label_mapping, scheduler, accumulation_steps=16\n","            )\n","            train_epoch_accuracies.append(train_epoch_accuracy)\n","\n","            # CosineAnnealingWarmRestarts scheduler.\n","            scheduler.step()\n","\n","            print(\"Testing...\")\n","            test_loss, test_accuracy = test_model(model, test_loader, device, label_mapping, criterion)\n","            test_epoch_accuracies.append(test_accuracy)\n","\n","            epoch_time = time.time() - epoch_start_time\n","            total_time = time.time() - total_start_time\n","\n","            print(f\"End of Epoch {epoch + 1} - Training Accuracy: {train_epoch_accuracy:.4f}, Training Loss: {train_epoch_loss:.4f}, Test Accuracy: {test_accuracy}, Test Loss: {test_loss:.4f}\")\n","            print(f\"Epoch Time: {epoch_time:.2f}s, Total Time: {total_time:.2f}s\")\n","\n","            if early_stopping_patience is not None:\n","                if test_accuracy \u003e best_test_accuracy:\n","                    best_test_accuracy = test_accuracy\n","\n","                    os.makedirs(save_dir, exist_ok=True)\n","                    new_best_model_path = os.path.join(save_dir, f\"best_model_epoch_{epoch + 1}.pth\")\n","                    torch.save({\n","                        'epoch': epoch + 1,\n","                        'model_state_dict': model.state_dict(),\n","                        'optimizer_state_dict': optimizer.state_dict(),\n","                        'scheduler_state_dict': scheduler.state_dict(),\n","                        'loss': train_epoch_loss,\n","                        'test_accuracy': test_accuracy,\n","                    }, new_best_model_path)\n","                    print(f\"New best model saved at {new_best_model_path}\")\n","\n","                    if best_model_path and best_model_path != new_best_model_path:\n","                        os.remove(best_model_path)\n","                        print(f\"Deleted previous best model at {best_model_path}\")\n","                    best_model_path = new_best_model_path\n","\n","                    patience_counter = 0\n","                else:\n","                    patience_counter += 1\n","                    print(f\"No improvement. Patience counter: {patience_counter}/{early_stopping_patience}\")\n","\n","                if patience_counter \u003e= early_stopping_patience:\n","                    final_time = time.time() - total_start_time\n","                    print(f\"Early stopping triggered at epoch {epoch + 1}. Best test accuracy: {best_test_accuracy}%\")\n","                    print(f\"Total training time: {final_time:.2f}s\")\n","                    patience_limit_model_path = os.path.join(save_dir, f\"patience_limit_model_epoch_{epoch + 1}.pth\")\n","                    torch.save({\n","                        'epoch': epoch + 1,\n","                        'model_state_dict': model.state_dict(),\n","                        'optimizer_state_dict': optimizer.state_dict(),\n","                        'scheduler_state_dict': scheduler.state_dict(),\n","                        'loss': train_epoch_loss,\n","                        'test_accuracy': test_accuracy,\n","                    }, patience_limit_model_path)\n","                    print(f\"Limit model saved at {patience_limit_model_path}\")\n","                    break\n","        else:\n","            final_time = time.time() - total_start_time\n","            print(f\"Total training time: {final_time:.2f}s\")\n","            epoch_limit_model_path = os.path.join(save_dir, f\"limit_model_epoch_{epoch}.pth\")\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'scheduler_state_dict': scheduler.state_dict(),\n","                'loss': train_epoch_loss,\n","                'test_accuracy': test_accuracy,\n","            }, epoch_limit_model_path)\n","            print(f\"Epoch limit model saved at {epoch_limit_model_path}\")\n","            break\n","\n","    return train_epoch_accuracies, test_epoch_accuracies"]},{"cell_type":"markdown","metadata":{"id":"8vJUlBNUZj93"},"source":["# LOAD THE SELECTED MODEL"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1737482697024,"user":{"displayName":"CEM OZAN DOGAN","userId":"13610049962498605450"},"user_tz":-180},"id":"Wylcb4r4Zjcl"},"outputs":[],"source":["def load_checkpoint(model, optimizer, scheduler, checkpoint_path, device):\n","    \"\"\"Loads a model and optimizer checkpoint from the given path.\"\"\"\n","    if os.path.exists(checkpoint_path):\n","        checkpoint = torch.load(checkpoint_path, map_location=device)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","        start_epoch = checkpoint['epoch']\n","        loss = checkpoint['loss']\n","        # test_accuracy = checkpoint['test_accuracy'] USE THIS IN LATER RUNS...\n","        print(f\"Loaded checkpoint from {checkpoint_path} (epoch {start_epoch}, loss {loss:.4f})\")\n","        return start_epoch, loss, False\n","        #print(f\"Loaded checkpoint from {checkpoint_path} (epoch {start_epoch}, loss {loss:.4f},\n","        #      test accuracy {test_accuracy:.4f})\")\n","        # return start_epoch, loss, test_accuracy USE THIS IN LATER RUNS...\n","    else:\n","        print(\"No checkpoint found, starting from scratch.\")\n","        return 0, None, True  # Start from epoch 0"]},{"cell_type":"markdown","metadata":{"id":"bGUTZQeH-y9Y"},"source":["# USE THIS IF MEMORY IS NOT ENOUGH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aD0ckAVYyUgK"},"outputs":[],"source":["# Configuration settings to add at the start of your training script\n","\"\"\"\n","def configure_memory_settings():\n","    # Enable memory efficient attention\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True\n","\n","    # Set memory allocator settings\n","    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n","\n","    # Enable gradient checkpointing for the model\n","    def enable_gradient_checkpointing(model):\n","        if hasattr(model, 'gradient_checkpointing_enable'):\n","            model.gradient_checkpointing_enable()\n","\n","    return enable_gradient_checkpointing\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"MvgXIBWraBM6"},"source":["# ViT Parameter Check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jl9bx0GzaBM6","outputId":"67b9e9e0-e2bb-4179-a490-274e10f149bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","vit_tiny_patch16_224:\n","  num_layers: 12\n","  embed_dim: 192\n","  num_heads: 3\n","  mlp_dim: 768\n","  mlp_ratio: 4.0\n","\n","vit_small_patch16_224:\n","  num_layers: 12\n","  embed_dim: 384\n","  num_heads: 6\n","  mlp_dim: 1536\n","  mlp_ratio: 4.0\n","\n","vit_base_patch16_224:\n","  num_layers: 12\n","  embed_dim: 768\n","  num_heads: 12\n","  mlp_dim: 3072\n","  mlp_ratio: 4.0\n"]}],"source":["\"\"\"\n","import timm\n","import torch\n","\n","def check_vit_layers(model_name):\n","    \"\"\"\"\"\"\n","    Check the number of transformer layers and other architecture details in a ViT model.\n","\n","    Args:\n","        model_name (str): Name of the ViT model from timm (e.g., 'vit_tiny_patch16_224')\n","\n","    Returns:\n","        dict: Dictionary containing model architecture details\n","    \"\"\"\"\"\"\n","    try:\n","        # Load model\n","        model = timm.create_model(model_name, pretrained=False)\n","\n","        # Get the number of heads from the first attention block\n","        num_heads = model.blocks[0].attn.num_heads\n","\n","        # Get architecture details\n","        details = {\n","            'num_layers': len(model.blocks),\n","            'embed_dim': model.embed_dim,\n","            'num_heads': num_heads,\n","            'mlp_dim': model.blocks[0].mlp.fc1.out_features,  # Size of first MLP layer\n","            'mlp_ratio': model.blocks[0].mlp.fc1.out_features / model.embed_dim  # Calculate MLP ratio\n","        }\n","\n","        return details\n","\n","    except Exception as e:\n","        raise RuntimeError(f\"Error checking model {model_name}: {str(e)}\")\n","\n","# Check different ViT variants\n","vit_variants = [\n","    'vit_tiny_patch16_224',\n","    'vit_small_patch16_224',\n","    'vit_base_patch16_224'\n","]\n","\n","for variant in vit_variants:\n","    try:\n","        details = check_vit_layers(variant)\n","        print(f\"\\n{variant}:\")\n","        for key, value in details.items():\n","            print(f\"  {key}: {value}\")\n","    except RuntimeError as e:\n","        print(f\"\\n{variant}: {str(e)}\")\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"0d22e1TkRB1J"},"source":["# RUNNER"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"YXfdQ1h2ZOGN"},"outputs":[{"name":"stdout","output_type":"stream","text":["Background training running...\n","Cuda is available: True\n","Using the device: cuda\n","Batch size: 4\n"]},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-18-b994965531ad\u003e:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(checkpoint_path, map_location=device)\n"]},{"name":"stdout","output_type":"stream","text":["Loaded checkpoint from /content/epoch_2_model (epoch 2, loss 2.8653)\n","Train data set size: 15518\n","Test data set size: 1666\n","\n","Epoch 3/100  Current Learning Rate: 0.000100\n","Training...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/100: 100%|| 3880/3880 [47:16\u003c00:00,  1.37it/s, loss=2.86, accuracy=13.6]"]},{"name":"stdout","output_type":"stream","text":["################## Train Epoch Summary [3/100]##################:\n","Train Average Loss: 2.8565\n","Accuracy: 13.57%\n","Time: 2836.32s\n","Testing...\n"]},{"name":"stderr","output_type":"stream","text":["\n","Testing: 100%|| 417/417 [01:55\u003c00:00,  3.62it/s, loss=2.89, accuracy=12.5]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Test Results for Epoch:\n","Total Samples: 1666\n","Correct Predictions: 208\n","Test Average Loss: 2.8852\n","Accuracy: 12.48%\n","End of Epoch 3 - Training Accuracy: 13.5713, Training Loss: 2.8565, Test Accuracy: 12.48499399759904, Test Loss: 2.8852\n","Epoch Time: 2951.60s, Total Time: 2951.60s\n","New best model saved at /content/drive/MyDrive/COMP411_Project_Models/1-layer-models/best_model_epoch_3.pth\n","\n","Epoch 4/100  Current Learning Rate: 0.000100\n","Training...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/100: 100%|| 3880/3880 [46:19\u003c00:00,  1.40it/s, loss=2.84, accuracy=14.4]"]},{"name":"stdout","output_type":"stream","text":["################## Train Epoch Summary [4/100]##################:\n","Train Average Loss: 2.8432\n","Accuracy: 14.43%\n","Time: 2779.41s\n","Testing...\n"]},{"name":"stderr","output_type":"stream","text":["\n","Testing: 100%|| 417/417 [01:51\u003c00:00,  3.73it/s, loss=2.87, accuracy=17.1]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Test Results for Epoch:\n","Total Samples: 1666\n","Correct Predictions: 285\n","Test Average Loss: 2.8683\n","Accuracy: 17.11%\n","End of Epoch 4 - Training Accuracy: 14.4348, Training Loss: 2.8432, Test Accuracy: 17.106842737094837, Test Loss: 2.8683\n","Epoch Time: 2891.46s, Total Time: 5843.68s\n","New best model saved at /content/drive/MyDrive/COMP411_Project_Models/1-layer-models/best_model_epoch_4.pth\n","Deleted previous best model at /content/drive/MyDrive/COMP411_Project_Models/1-layer-models/best_model_epoch_3.pth\n","\n","Epoch 5/100  Current Learning Rate: 0.000100\n","Training...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/100: 100%|| 3880/3880 [46:14\u003c00:00,  1.40it/s, loss=2.82, accuracy=15.8]"]},{"name":"stdout","output_type":"stream","text":["################## Train Epoch Summary [5/100]##################:\n","Train Average Loss: 2.8245\n","Accuracy: 15.84%\n","Time: 2774.56s\n","Testing...\n"]},{"name":"stderr","output_type":"stream","text":["\n","Testing: 100%|| 417/417 [01:51\u003c00:00,  3.75it/s, loss=2.89, accuracy=13.4]"]},{"name":"stdout","output_type":"stream","text":["\n","Test Results for Epoch:\n","Total Samples: 1666\n","Correct Predictions: 223\n","Test Average Loss: 2.8934\n","Accuracy: 13.39%\n","End of Epoch 5 - Training Accuracy: 15.8397, Training Loss: 2.8245, Test Accuracy: 13.385354141656663, Test Loss: 2.8934\n","Epoch Time: 2885.96s, Total Time: 8730.23s\n","No improvement. Patience counter: 1/20\n","\n","Epoch 6/100  Current Learning Rate: 0.000099\n","Training...\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 6/100: 100%|| 3880/3880 [46:04\u003c00:00,  1.40it/s, loss=2.82, accuracy=16.2]"]},{"name":"stdout","output_type":"stream","text":["################## Train Epoch Summary [6/100]##################:\n","Train Average Loss: 2.8171\n","Accuracy: 16.21%\n","Time: 2764.71s\n","Testing...\n"]},{"name":"stderr","output_type":"stream","text":["\n","Testing: 100%|| 417/417 [01:54\u003c00:00,  3.63it/s, loss=2.92, accuracy=12.8]"]},{"name":"stdout","output_type":"stream","text":["\n","Test Results for Epoch:\n","Total Samples: 1666\n","Correct Predictions: 213\n","Test Average Loss: 2.9196\n","Accuracy: 12.79%\n","End of Epoch 6 - Training Accuracy: 16.2134, Training Loss: 2.8171, Test Accuracy: 12.785114045618249, Test Loss: 2.9196\n","Epoch Time: 2879.74s, Total Time: 11609.97s\n","No improvement. Patience counter: 2/20\n","\n","Epoch 7/100  Current Learning Rate: 0.000099\n","Training...\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 7/100: 100%|| 3880/3880 [46:28\u003c00:00,  1.39it/s, loss=2.8, accuracy=17.4]"]},{"name":"stdout","output_type":"stream","text":["################## Train Epoch Summary [7/100]##################:\n","Train Average Loss: 2.8011\n","Accuracy: 17.36%\n","Time: 2788.94s\n","Testing...\n"]},{"name":"stderr","output_type":"stream","text":["\n","Testing: 100%|| 417/417 [01:50\u003c00:00,  3.76it/s, loss=2.89, accuracy=14.8]"]},{"name":"stdout","output_type":"stream","text":["\n","Test Results for Epoch:\n","Total Samples: 1666\n","Correct Predictions: 247\n","Test Average Loss: 2.8902\n","Accuracy: 14.83%\n","End of Epoch 7 - Training Accuracy: 17.3605, Training Loss: 2.8011, Test Accuracy: 14.82593037214886, Test Loss: 2.8902\n","Epoch Time: 2900.02s, Total Time: 14509.99s\n","No improvement. Patience counter: 3/20\n","\n","Epoch 8/100  Current Learning Rate: 0.000099\n","Training...\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 8/100: 100%|| 3880/3880 [47:30\u003c00:00,  1.36it/s, loss=2.79, accuracy=17.8]"]},{"name":"stdout","output_type":"stream","text":["################## Train Epoch Summary [8/100]##################:\n","Train Average Loss: 2.7866\n","Accuracy: 17.84%\n","Time: 2850.17s\n","Testing...\n"]},{"name":"stderr","output_type":"stream","text":["\n","Testing: 100%|| 417/417 [01:55\u003c00:00,  3.60it/s, loss=2.93, accuracy=12.8]"]},{"name":"stdout","output_type":"stream","text":["\n","Test Results for Epoch:\n","Total Samples: 1666\n","Correct Predictions: 213\n","Test Average Loss: 2.9264\n","Accuracy: 12.79%\n","End of Epoch 8 - Training Accuracy: 17.8438, Training Loss: 2.7866, Test Accuracy: 12.785114045618249, Test Loss: 2.9264\n","Epoch Time: 2966.25s, Total Time: 17476.24s\n","No improvement. Patience counter: 4/20\n","\n","Epoch 9/100  Current Learning Rate: 0.000098\n","Training...\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 9/100: 100%|| 3880/3880 [47:16\u003c00:00,  1.37it/s, loss=2.77, accuracy=18.9]"]},{"name":"stdout","output_type":"stream","text":["################## Train Epoch Summary [9/100]##################:\n","Train Average Loss: 2.7670\n","Accuracy: 18.91%\n","Time: 2836.46s\n","Testing...\n"]},{"name":"stderr","output_type":"stream","text":["\n","Testing: 100%|| 417/417 [01:51\u003c00:00,  3.75it/s, loss=2.86, accuracy=14.8]"]},{"name":"stdout","output_type":"stream","text":["\n","Test Results for Epoch:\n","Total Samples: 1666\n","Correct Predictions: 246\n","Test Average Loss: 2.8555\n","Accuracy: 14.77%\n","End of Epoch 9 - Training Accuracy: 18.9071, Training Loss: 2.7670, Test Accuracy: 14.765906362545017, Test Loss: 2.8555\n","Epoch Time: 2947.86s, Total Time: 20424.10s\n","No improvement. Patience counter: 5/20\n","\n","Epoch 10/100  Current Learning Rate: 0.000098\n","Training...\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 10/100: 100%|| 3880/3880 [47:30\u003c00:00,  1.36it/s, loss=2.76, accuracy=18.9]"]},{"name":"stdout","output_type":"stream","text":["################## Train Epoch Summary [10/100]##################:\n","Train Average Loss: 2.7608\n","Accuracy: 18.89%\n","Time: 2850.31s\n","Testing...\n"]},{"name":"stderr","output_type":"stream","text":["\n","Testing: 100%|| 417/417 [01:51\u003c00:00,  3.75it/s, loss=2.93, accuracy=12.5]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Test Results for Epoch:\n","Total Samples: 1666\n","Correct Predictions: 208\n","Test Average Loss: 2.9260\n","Accuracy: 12.48%\n","End of Epoch 10 - Training Accuracy: 18.8942, Training Loss: 2.7608, Test Accuracy: 12.48499399759904, Test Loss: 2.9260\n","Epoch Time: 2961.46s, Total Time: 23385.56s\n","No improvement. Patience counter: 6/20\n","Total training time: 23385.56s\n","Epoch limit model saved at /content/drive/MyDrive/COMP411_Project_Models/1-layer-models/limit_model_epoch_10.pth\n","Train Accuracies: [13.57133651243717, 14.434849851785023, 15.839670060574818, 16.21342956566568, 17.360484598530736, 17.84379430338961, 18.907075654079133, 18.894187395282895]\n","Test Accuracies: [12.48499399759904, 17.106842737094837, 13.385354141656663, 12.785114045618249, 14.82593037214886, 12.785114045618249, 14.765906362545017, 12.48499399759904]\n"]}],"source":["# Learning Rate Scheduling\n","from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n","from transformers import get_cosine_schedule_with_warmup\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","import torchvision.transforms as transforms\n","\n","\n","if __name__ == \"__main__\":\n","    print(\"Background training running...\")\n","    import os\n","    from pathlib import Path\n","\n","    # Check for CUDA/device availability\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Cuda is available: {torch.cuda.is_available()}\")\n","    print(f\"Using the device: {device}\")\n","    \"\"\"\n","    # Simpler architecture\n","    tubelet_sizes = [(16, 16, 16), (32, 16, 16), (64, 16, 16)]\n","    embed_dims_per_view = [256, 128, 64]\n","    MLP_dims_per_view = [1024, 512, 256]\n","    num_layers_per_view = [3, 3, 3]\n","    view_dropout_list = [0.3, 0.2, 0.1]\n","    cross_view_connections = [1,2]\n","    global_embed_dim = 72\n","    global_num_layers = 1\n","    num_classes = 174\n","    \"\"\"\n","    # [(16, 16, 16), (32, 16, 16), (64, 16, 16)] Later [(4, 16, 16), (8, 16, 16), (16, 16, 16)]\n","    # Hyperparameters # Use at least 3 views, less views decreases models capacity a lot. [(64, 64, 64), (32, 32, 32), (16, 16, 16)]\n","    tubelet_sizes = [(4, 16, 16), (8, 16, 16), (12, 16, 16)] # (Temporal, Height, Width) sizes oldest=[(8, 8, 8), (4, 4, 4), (2, 2, 2)] old=[(32, 32, 32), (16, 16, 16)] ver_3=[(64, 64, 64), (32, 32, 32), (16, 16, 16)]\n","    embed_dims_per_view = [768, 384, 192]       # Embedding dimensions for each view (Base, Small, Tiny) [768, 512, 256] [384, 256] [64, 32, 16]\n","    MLP_dims_per_view = [768*4, 384*4, 192*4]       # For view MLP dims. [256, 128, 64]\n","    num_of_head_per_view = [12, 6, 3]\n","    #embed_dims_pairs_for_cross_att = [(512,256),(512,256),(256,128),(256,128)]  # [768, 512, 256] [384, 256] [512, 256, 128]\n","    num_layers_per_view = [1, 1, 1]   # Number of transformer layers per view ver_1=[12, 8, 4]. ver_2=[12, 8]. ver_3=[4, 2] ver_4=[2, 1] ver_5=[4, 2, 1] ver_6=[4, 4, 4] [1, 1, 1]\n","    view_dropout_list = [0.2, 0.15, 0.1] # view0, view1, view2 ... Bigger views might memorize more than smaller ones. [0.2, 0.1, 0.05] [0.4, 0.3, 0.25]\n","    #(1, 2) means connect the 2nd layer of view\n","    #X to the 2nd layer of view X+1.\n","    #(2, 2) means connect the 3rd layer of view\n","    #X to the 2nd layer of view X+1. 0,1\n","    cross_view_connections = [0]     # Cross-view attention connections. The first view layer is named \"0\" for better readability. [0, 1]\n","    global_embed_dim = 192             # Global encoder dimension. ver_0=256, ver_1=192 ver_2=144\n","    global_num_layers = 1              # Number of global transformer blocks. Normally it was 4.\n","    num_classes = 174                  # Total classes in Something-Something V2. CHANGE THIS TO 10 FOR MAINDATASET OR IT CAN REMAIN 174. ver_0=174\n","\n","    \"\"\"\n","    tubelet_sizes = [(64, 64, 64), (32, 32, 32), (16, 16, 16)]\n","    embed_dims_per_view = [512, 384, 256]\n","    embed_dims_per_cross_att = [512, 384, 256]\n","    num_layers_per_view = [3,2,1]\n","    global_embed_dim = 512\n","    global_num_layers = 2\n","    num_classes = 174\n","    \"\"\"\n","\n","    # Initialize model. Add gradient checkpointing to your model initialization.\n","    # Use the commented if memory is not enough.\n","    #enable_gradient_checkpointing = configure_memory_settings()\n","    model = MultiviewTransformer(device,\n","                                 embed_dims_per_view,\n","                                 MLP_dims_per_view,\n","                                 num_of_head_per_view,\n","                                 view_dropout_list,\n","                                 num_layers_per_view,\n","                                 tubelet_sizes,\n","                                 global_embed_dim,\n","                                 global_num_layers,\n","                                 num_classes,\n","                                 cross_view_connections).to(device)\n","    #enable_gradient_checkpointing(model)\n","    \"\"\"\n","    for name, param in model.named_parameters():\n","      print(f\"Parameter: {name} | Device: {param.device}\")\n","    \"\"\"\n","    \"\"\"\n","    for name, module in model.named_modules():\n","      print(f\"Module: {name} | Device: {next(module.parameters()).device}\")\n","    \"\"\"\n","    \"\"\"\n","    for name, module in model.named_modules():\n","      print(f\"Module: {name} | Device: {next(module.parameters()).device}\")\n","    \"\"\"\n","\n","    # Dataset and video directory paths\n","    # video_dir = \"/content/drive/MyDrive/COMP411_Project_Datasets/Something-Something-V2/20bn-something-something-v2\"\n","    video_dir_train = \"/content/dataset/Top-10-Label-Dataset/train\"\n","    video_dir_test = \"/content/dataset/Top-10-Label-Dataset/test\"\n","    #video_dir = \"/kaggle/input/test-10/test10\"\n","\n","    # Dataset transformations (if any)\n","    \"\"\"\n","    video_transform = transforms.Compose([\n","        #transforms.ToPILImage(),  # Convert frame to PIL Image (needed for transformations)\n","        #transforms.Resize((224, 224)),  # Resize each frame to the desired size\n","        #transforms.RandomHorizontalFlip(),  # Apply random horizontal flip for data augmentation\n","        transforms.ToTensor(),  # Convert frame to tensor (required for PyTorch models)\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n","    ])\n","    \"\"\"\n","    \"\"\"\n","    video_transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(0.2, 0.2, 0.2),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                            std=[0.229, 0.224, 0.225])\n","    ])\n","    \"\"\"\n","    \"\"\"\n","    train_frame_transform = transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(0.2, 0.2, 0.2),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    test_transform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])\n","    ])\n","    \"\"\"\n","    # Initialize datasets\n","    # train_data and test_data are created in \"GET THE DATA LABELS\" part.\n","    totalViews = len(embed_dims_per_view)\n","    train_dataset = TwentyBillionSomethingDataset(\n","        video_dir=video_dir_train,\n","        data_set=train_data,\n","        label_mapping=label_mapping,\n","        totalViews=totalViews,\n","        target_frames=48,  # Adjust based on your needs\n","        target_size=(224, 224),\n","        is_test=False,\n","        test_answers_dict=None\n","        )\n","    test_dataset = TwentyBillionSomethingDataset(\n","        video_dir=video_dir_test,\n","        data_set=test_data,\n","        label_mapping=label_mapping,\n","        totalViews=totalViews,\n","        target_frames=48,  # Adjust based on your needs\n","        target_size=(224, 224),\n","        is_test=True,\n","        test_answers_dict=test_answers\n","        )\n","\n","    \"\"\"\n","    def get_optimal_batch_size(base_batch_size):\n","      if(torch.cuda.is_available()):\n","        total_memory = torch.cuda.get_device_properties(0).total_memory\n","        if total_memory \u003e 16*1024*1024*1024:  # \u003e16GB\n","            return base_batch_size * 2\n","        elif total_memory \u003e 8*1024*1024*1024:  # \u003e8GB\n","            return base_batch_size\n","        else:\n","            return base_batch_size // 2\n","      else:\n","        return base_batch_size\n","    \"\"\"\n","    #batch_size = get_optimal_batch_size(16)\n","    batch_size = 4\n","    print(\"Batch size: \" + str(batch_size))\n","\n","    # DataLoader configurations\n","    \"\"\"\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","    \"\"\"\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=4,\n","        pin_memory=True,  # Add this for faster GPU transfer\n","        prefetch_factor=2  # Each worker prefetches 2 batches\n","        )\n","    test_loader = DataLoader(\n","        test_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=4,\n","        pin_memory=True,\n","        prefetch_factor=2\n","        )\n","\n","    # Model save directory.\n","    # Write here the model save folder. Background-Runner-Test-1\n","    #model_save_dir = \"/kaggle/temp/COMP411_Project_Models/ActionRecognizerModels_20k\"\n","    model_save_dir = \"/content/drive/MyDrive/COMP411_Project_Models/1-layer-models\"\n","\n","    # USE THIS IF THERE IS A SAVED MODEL.\n","    #SELECT_SAVED_MODEL_TO_USE_EPOCH = 5\n","\n","    # Create save directory for models if it doesn't exist.\n","    Path(model_save_dir).mkdir(parents=True, exist_ok=True)\n","\n","    # Model checkpoint file.\n","    # CHANGE THE MODEL FILE NAME TO CONTINUE FROM THAT MODEL. best_model_epoch_5.pth\n","    #checkpoint_path = os.path.join(model_save_dir, (\"best_model_epoch_\" + str(SELECT_SAVED_MODEL_TO_USE_EPOCH) + \".pth\"))\n","    checkpoint_path = \"/content/epoch_2_model\"\n","\n","    # Training parameters\n","    total_num_epochs = 100\n","    current_model_epoch = 0 # SET THIS EVERY TIME YOU RUN THE RUNNER.##############################\n","    limit_epoch = 10 # IF CREDIT FOR GPU NOT ENOUGH, DO THE RUN FOR A SMALLER TIME.\n","    # save_per_epoch = 5 # CHANGE THIS TO 5 OR Something.\n","    early_stopping_patience = 20\n","\n","    # Loss function and optimizer\n","    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n","\n","    # Modify optimizer. USE 1e-3 for weight decay it is better for transformers.\n","    \"\"\"\n","    optimizer = optim.AdamW(model.parameters(),\n","                            lr=1e-4,  # Increased learning rate. ver_0=1e-3 ver_1=5e-4 ver_2=2e-4\n","                            weight_decay=1e-3,  # Make it different(1e-3) than 0 to generalize to data. ver_0=1e-4\n","                            betas=(0.9, 0.999))  # Default AdamW betas\n","    \"\"\"\n","\n","    optimizer = optim.AdamW(\n","    model.parameters(),\n","    lr=1e-4,  # Slightly higher\n","    weight_decay=0.001,\n","    betas=(0.9, 0.999)\n","    )\n","    scheduler = CosineAnnealingLR(optimizer, T_max=total_num_epochs, eta_min=1e-7)\n","    \"\"\"\n","    # Warmup scheduler\n","    num_warmup_steps = len(train_loader) * 5  # 5 epochs warmup\n","    total_training_steps = len(train_loader) * total_num_epochs\n","    scheduler = get_cosine_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=total_training_steps\n","        )\n","    \"\"\"\n","    #scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n","    # 0-5 6-15 16-35 36-75\n","\n","    # MIGHT RETURN TO COSINE ANNEALING. IT WORKS GOOD TOO.\n","    \"\"\"\n","    scheduler = OneCycleLR(\n","        optimizer,\n","        max_lr=2e-4,\n","        total_steps=limit_epoch * len(train_loader),\n","        anneal_strategy='cos',\n","        div_factor=10,\n","        final_div_factor=1e4,\n","        pct_start=0.2\n","    )\n","    \"\"\"\n","    \"\"\"\n","    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n","    \"\"\"\n","    \"\"\"\n","    scheduler = OneCycleLR(\n","    optimizer,\n","    max_lr=5e-4,  # Lower than single-view due to multiple parallel paths\n","    total_steps=(limit_epoch - current_model_epoch) * len(train_loader), # Update the current epoch if you continue training from earlier.\n","    anneal_strategy='cos',\n","    div_factor=5,\n","    final_div_factor=1e3,\n","    pct_start=0.15  # Slightly longer warmup for multi-view stability\n","    )\n","    \"\"\"\n","    \"\"\"\n","    # This is for 100 epochs.\n","    scheduler = OneCycleLR(\n","    optimizer,\n","    max_lr=5e-4,  # Lower than single-view due to multiple parallel paths\n","    total_steps=total_num_epochs * len(train_loader), # Update the current epoch if you continue training from earlier.\n","    anneal_strategy='cos',\n","    div_factor=5,\n","    final_div_factor=1e3,\n","    pct_start=0.15  # Slightly longer warmup for multi-view stability\n","    )\n","    \"\"\"\n","    \"\"\" FIND THE BEST HYPERPARAMETERS FOR THIS AND USE THIS.\n","    scheduler = get_cosine_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=warmup_steps,\n","        num_training_steps=total_training_steps\n","        )\n","    \"\"\"\n","    \"\"\"\n","    # More aggressive learning rate scheduling\n","    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","        optimizer,\n","        max_lr=5e-3,  # Higher peak learning rate\n","        steps_per_epoch=len(train_loader),\n","        epochs=total_num_epochs\n","        )\n","    \"\"\"\n","    # LOAD MODEL CHECKPOINT\n","    start_epoch, _, use_pretrained_weights = load_checkpoint(model, optimizer,\n","                                                             scheduler, checkpoint_path,\n","                                                             device) # get last trained epoch and start from the next epoch\n","\n","    \"\"\"\n","    start_epoch, _, last_best_test_accuracy = load_checkpoint(model, optimizer,\n","                                                              scheduler, checkpoint_path,\n","                                                              device) # get last trained epoch and start from the next epoch\n","    \"\"\"\n","\n","    # Train and test the model\n","    print(\"Train data set size: \" + str(len(train_loader.dataset)))\n","    print(\"Test data set size: \" + str(len(test_loader.dataset)))\n","\n","    train_acc, test_acc = train_then_test_for_each_epoch(\n","        model, train_loader, criterion, optimizer, device,\n","        total_num_epochs, test_loader, model_save_dir, label_mapping,\n","        scheduler, None, start_epoch, limit_epoch, early_stopping_patience,\n","        use_pretrained_weights\n","    )\n","    \"\"\" USE THIS AFTER THE FIRST RUN.\n","    train_acc, test_acc = train_then_test_for_each_epoch(\n","        model, train_loader, criterion, optimizer, device,\n","        total_num_epochs, test_loader, model_save_dir, label_mapping,\n","        scheduler, None, start_epoch, limit_epoch, last_best_test_accuracy,\n","        early_stopping_patience\n","    )\n","    \"\"\"\n","    \"\"\"\n","    train_acc, test_acc = train_then_test_for_each_epoch(\n","        model, train_loader, criterion, optimizer, device,\n","        total_num_epochs, test_loader, model_save_dir, label_mapping,\n","        scheduler, save_per_epoch, start_epoch, early_stopping_patience\n","    )\n","    \"\"\"\n","\n","    # Print final accuracies\n","    print(\"Train Accuracies:\", train_acc)\n","    print(\"Test Accuracies:\", test_acc)"]},{"cell_type":"markdown","metadata":{"id":"1yV0UCRWIcJl"},"source":["# LIBRARIES(AGAIN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXkKg3gJIcZw"},"outputs":[],"source":["\"\"\"\n","import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","import time  # Import time module for tracking time.\n","import torch.multiprocessing as mp\n","import cv2 # For dividing the video into frames.\n","from tqdm.notebook import tqdm # For better visuals while training the model.\n","from torch import Tensor\n","from typing import Optional, Tuple\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"aMRT5ajXITcg"},"source":["# CLEAR RAM MEMORY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZCLuCYQITs7"},"outputs":[],"source":["\"\"\"\n","import psutil\n","import os\n","\n","def clear_system_memory():\n","    # On Unix-like systems (Linux/macOS)\n","    if os.name == 'posix':\n","        os.system('sync; echo 3 \u003e /proc/sys/vm/drop_caches')\n","\n","    # Print memory usage\n","    memory = psutil.virtual_memory()\n","    print(f\"Total Memory: {memory.total / (1024**3):.2f} GB\")\n","    print(f\"Available Memory: {memory.available / (1024**3):.2f} GB\")\n","    print(f\"Used Memory: {memory.used / (1024**3):.2f} GB\")\n","\n","\n","# Call this function to clear memory\n","clear_system_memory()\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3oWSOu9JJIY"},"outputs":[],"source":["\"\"\"\n","import psutil\n","import os\n","\n","def clear_ram():\n","    # Force Python to release memory\n","    import ctypes\n","    libc = ctypes.CDLL('libc.so.6')\n","    libc.malloc_trim(0)\n","\n","    # Print memory before and after\n","    print(\"Memory before clearing:\", psutil.virtual_memory().percent, \"%\")\n","\n","    # Garbage collection\n","    import gc\n","    gc.collect()\n","\n","    print(\"Memory after clearing:\", psutil.virtual_memory().percent, \"%\")\n","\n","# Call the function\n","clear_ram()\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"WJFva__8hZn4"},"source":["# MAX TRAIN AND TEST ACCURACIES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cIHFy6YZhZOV"},"outputs":[],"source":["if train_acc:\n","  print(\"Max Train Accuracy: \" + str(max(train_acc)) + \" at Epoch: \" + str(train_acc.index(max(train_acc)) + 1))\n","if test_acc:\n","  print(\"Max Test Accuracy: \" + str(max(test_acc)) + \" at Epoch: \" + str(test_acc.index(max(test_acc)) + 1))"]},{"cell_type":"markdown","metadata":{"id":"-W6wEpoNNFVd"},"source":["# PLOT THE ACCURACIES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHjgR8trM6Iv"},"outputs":[],"source":["\"\"\"\n","import matplotlib.pyplot as plt\n","\n","epochs = range(1, len(train_acc) + 1)\n","\n","plt.plot(epochs, train_acc, label='Training Accuracy')\n","#plt.plot(epochs, test_acc, label='Test Accuracy')  # Uncomment if you have test accuracy data\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Training Accuracy over Epochs')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"4DGLvmh3sLAe"},"source":["#CLEAR GPU MEMORY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6LC2HPemUHE"},"outputs":[],"source":["#from IPython import get_ipython\n","\n","# Clear all variables\n","#get_ipython().magic('reset -sf')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UtFBfXNdsKiU"},"outputs":[],"source":["#torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ArY7pOFssrPM"},"outputs":[],"source":["#print(torch.cuda.memory_summary())"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","name":"","version":""},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6352920,"sourceId":10268370,"sourceType":"datasetVersion"},{"datasetId":6388509,"sourceId":10318772,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}